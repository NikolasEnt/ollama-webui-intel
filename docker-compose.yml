# Ollama with ipex (Intel iGPU acceleration) and
services:
  ipex_ollama:
    image: ipex_ollama:0.1.0
    build:
      dockerfile: ./Dockerfile
    container_name: ipex_ollama
    healthcheck:
      test: ollama --version || exit 1
      interval: 60s
    ports:
      - 11434:11434 # Expose port to use outside of webui
    volumes:
      - ./data/models:/models # Change to use another location for model storage
      - ./data/logs:/logs
    devices:
      - /dev/dri:/dev/dri
    environment:
      - OLLAMA_KEEP_ALIVE=12h
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=1 # Number of parallel request Ollama handle
      - OLLAMA_MODELS=/models # Path to models storage
      - PATH=/llm/ollama:$PATH
    command: bash -c "/llm/ollama/ollama serve > /logs/ollama.log"
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:v0.5.16
    ports:
      - 18080:8080
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./data/ollama-webui:/app/backend/data
    restart: always
    depends_on:
      ipex_ollama:
        condition: service_started
        restart: true
    environment:
      - OLLAMA_BASE_URL=http://ipex_ollama:11434
